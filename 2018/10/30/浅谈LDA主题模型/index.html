<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/xiaoming-48x48.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/xiaoming-32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/xiaoming-16x16.ico">
  <link rel="mask-icon" href="/images/xiaoming-48x48.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.2/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"seanlee97.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.2.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}};
  </script>
<meta name="description" content="首先声明，这里的 LDA 是指 Latent Dirichlet Allocation 隐含狄利克雷分布，而不是 Linear Discriminant Analysis 线性判别分析 (笔者有幸在 City University of HK 听过一堂机器学习课，里面讲到了线性判别，受益匪浅，有机会再做分享) 除了看原论文 Latent Dirichlet Allocation , 虽然论文一作不">
<meta property="og:type" content="article">
<meta property="og:title" content="浅谈LDA主题模型(原理篇)">
<meta property="og:url" content="https://seanlee97.github.io/2018/10/30/%E6%B5%85%E8%B0%88LDA%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="明天探索者">
<meta property="og:description" content="首先声明，这里的 LDA 是指 Latent Dirichlet Allocation 隐含狄利克雷分布，而不是 Linear Discriminant Analysis 线性判别分析 (笔者有幸在 City University of HK 听过一堂机器学习课，里面讲到了线性判别，受益匪浅，有机会再做分享) 除了看原论文 Latent Dirichlet Allocation , 虽然论文一作不">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://seanlee97.github.io/images/posts/lda_topic/topic-words.png">
<meta property="og:image" content="https://seanlee97.github.io/images/posts/lda_topic/gamma-distribution.png">
<meta property="og:image" content="https://seanlee97.github.io/images/posts/lda_topic/dirichlet-alpha.gif">
<meta property="og:image" content="https://seanlee97.github.io/images/posts/lda_topic/corpus.png">
<meta property="og:image" content="https://seanlee97.github.io/images/posts/lda_topic/bayes-model.png">
<meta property="article:published_time" content="2018-10-30T13:55:00.000Z">
<meta property="article:modified_time" content="2021-03-07T10:05:36.806Z">
<meta property="article:author" content="Sean Lee">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://seanlee97.github.io/images/posts/lda_topic/topic-words.png">


<link rel="canonical" href="https://seanlee97.github.io/2018/10/30/%E6%B5%85%E8%B0%88LDA%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>浅谈LDA主题模型(原理篇) | 明天探索者</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">明天探索者</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#lda-%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="nav-number">1.</span> <span class="nav-text">LDA 的解释</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86"><span class="nav-number">2.</span> <span class="nav-text">背景知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gamma-%E5%87%BD%E6%95%B0"><span class="nav-number">2.1.</span> <span class="nav-text">Gamma 函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#gamma-%E5%88%86%E5%B8%83"><span class="nav-number">2.1.1.</span> <span class="nav-text">Gamma 分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8B%93gamma-%E5%88%86%E5%B8%83%E4%B8%8E-poisson-%E5%88%86%E5%B8%83"><span class="nav-number">2.1.2.</span> <span class="nav-text">【拓】Gamma 分布与 Poisson 分布</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C%E5%88%86%E5%B8%83"><span class="nav-number">2.2.</span> <span class="nav-text">共轭先验分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E5%92%8C-beta-%E5%88%86%E5%B8%83"><span class="nav-number">2.3.</span> <span class="nav-text">二项分布和 Beta 分布</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#beta%E5%88%86%E5%B8%83%E7%9A%84%E6%9C%9F%E6%9C%9B"><span class="nav-number">2.3.1.</span> <span class="nav-text">Beta分布的期望</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83%E5%92%8C-dirichlet-%E5%88%86%E5%B8%83"><span class="nav-number">2.4.</span> <span class="nav-text">多项分布和 Dirichlet 分布</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E7%A7%B0-dirichlet-%E5%88%86%E5%B8%83"><span class="nav-number">2.4.1.</span> <span class="nav-text">对称 Dirichlet 分布</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A1%AC-%E8%BD%AF%E6%80%9D%E7%BB%B4"><span class="nav-number">2.5.</span> <span class="nav-text">硬 &amp; 软思维</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%8D%E8%AF%9D-lda"><span class="nav-number">3.</span> <span class="nav-text">再话 LDA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#refrence"><span class="nav-number">4.</span> <span class="nav-text">Refrence</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Sean Lee"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Sean Lee</p>
  <div class="site-description" itemprop="description">一枚小小的程序员</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://seanlee97.github.io/2018/10/30/%E6%B5%85%E8%B0%88LDA%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Sean Lee">
      <meta itemprop="description" content="一枚小小的程序员">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="明天探索者">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          浅谈LDA主题模型(原理篇)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2018-10-30 21:55:00" itemprop="dateCreated datePublished" datetime="2018-10-30T21:55:00+08:00">2018-10-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-03-07 18:05:36" itemprop="dateModified" datetime="2021-03-07T18:05:36+08:00">2021-03-07</time>
      </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>首先声明，这里的 LDA 是指 Latent Dirichlet Allocation 隐含狄利克雷分布，而不是 Linear Discriminant Analysis 线性判别分析 (笔者有幸在 City University of HK 听过一堂机器学习课，里面讲到了线性判别，受益匪浅，有机会再做分享)</p>
<p>除了看原论文 <a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Latent Dirichlet Allocation</a> , 虽然论文一作不是安德鲁(吴恩达)，但是看到他的名字就知道这篇论文的水平，我也看了很多博客，写得最好的是<code>火光摇曳</code>的 《LDA数学八卦》，本文部分知识来自此篇博客。</p>
<h2 id="lda-的解释">LDA 的解释</h2>
<p>既然 LDA 是主题模型，那么它的功能应该是和 LSA、 pLSA 等差不多的。它们都可以得到文档的主题，以及主题词。LSA 一般通过 SVD 求解（可参考先前的文章 <a href="https://seanlee97.github.io/2018/09/01/SVD%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8ALSA%E7%9A%84%E6%B1%82%E8%A7%A3/">SVD的原理及LSA的求解</a>），而 pLSA 和 LDA 是概率模型。</p>
<p>那么什么是LDA呢？</p>
<p>假设有 M 篇文章，我们假设每篇文章中隐含有 K 个主题。其中文章和文章的词是可见的，而主题是假设出来的，且主题往往是受词影响的，可以把这些主题称为 <code>latent topics</code> 隐主题。</p>
<p><img src="/images/posts/lda_topic/topic-words.png" /></p>
<p>假设每篇文章的长度为 N (即是每篇文章有 N 个词)，每篇文章都有各自的主题分布（是软分布），主题分布是<code>多项分布</code>， 该多项分布的参数服从<code>Dirichlet 分布</code>, 该<code>Dirichlet 分布</code>的参数为 <span class="math inline">\(\alpha\)</span></p>
<p>每个主题都有各自的词分布，词分布也是<code>多项分布</code>，其多项分布的参数也同样服从<code>Dirichlet 分布</code>，该<code>Dirichlet 分布</code>的参数为 <span class="math inline">\(\beta\)</span></p>
<p>通俗地说就是对于某篇文章中的第 n 个词，首先要从这篇文章的主题分布中采样一个主题，然后再在这个主题对应的词分布中采样一个词，然后不断重复这个过程，直到 m 篇文章全部完成上述过程。</p>
<p>而 LDA 是基于贝叶斯模型的，是一个三层的网络，贝叶斯模型有三个主要部分：先验分布、数据似然、后验分布，它们的关系为</p>
<p align="center">
先验分布 + 数据似然 = 后验分布
</p>
<p><strong>为了让上述关系满足递推关系</strong>，LDA 一般采用的分布是满足共轭分布的，共轭分布的先验分布和后验分布满足相同的分布律，<strong>即当前的求得的后验分布可作为下一次的先验分布</strong>。</p>
<p>上面内容先介绍了 LDA 的主要思想，下面主要介绍 LDA 的细节</p>
<h2 id="背景知识">背景知识</h2>
<h3 id="gamma-函数">Gamma 函数</h3>
<p><code>Beta 分布</code> 和 <code>Dirichlet 分布</code> 中都用到 Gamma 函数 <span class="math inline">\(\Gamma(\alpha)\)</span> ,它的定义为：</p>
<p><span class="math display">\[\begin{align*}
&amp; \Gamma(\alpha) = \int_{0}^{+\infty } x^{\alpha-1} e^{-x} dx \\
&amp; where\ \ x &gt; 0
\end{align*} \tag{1}\]</span></p>
<p>另 <span class="math inline">\(x = t^{2}\)</span> 带回 <span class="math inline">\((1)\)</span> 式可得到 Gamma 函数的另一表达形式</p>
<p><span class="math display">\[\begin{align*}
&amp; \Gamma(\alpha) = 2 \int_{0}^{+ \infty} t^{2\alpha-1} e^{-t^{2}} dt \\
&amp; where\ \ t &gt; 0
\end{align*} \tag{2}\]</span></p>
<p>对于 <span class="math inline">\((1)\)</span> 式，将 <span class="math inline">\(\alpha = 1\)</span> 带入求积分易求得:</p>
<p><span class="math display">\[\Gamma(1) = 1 \tag{3}\]</span></p>
<p>对于 <span class="math inline">\((2)\)</span> 式，将 <span class="math inline">\(\alpha = \frac{1}{2}\)</span> 带入求积分可得：</p>
<p><span class="math display">\[\Gamma(\frac{1}{2}) = \sqrt{\pi} \tag{4}\]</span></p>
<p>接下来我们用分部积分法求解可得：</p>
<p><span class="math display">\[\begin{align*}
\Gamma(\alpha + 1) &amp; = \int _{0}^{+\infty} x^{\alpha} e^{-x} dx \\
&amp; = -\int _{0}^{+\infty} x^{\alpha}d e^{-x} \\
&amp; = -x^{\alpha} e^{-x} | _{0}^{+\infty} + \int _{0}^{+\infty} e^{-x}\alpha x^{\alpha-1} dx \\
&amp; = \alpha \int _{0}^{\infty} x^{\alpha - 1} e^{-x} dx \\
&amp; = \alpha \Gamma(\alpha)
\end{align*} \tag{5}\]</span></p>
<p>因此</p>
<p><span class="math display">\[\Gamma(\alpha + 1) = \alpha \Gamma(\alpha) \tag{6}\]</span></p>
<p>易知 <span class="math inline">\(\Gamma(\alpha)\)</span> 函数是阶乘在实数集上的扩展，具有如下性质：</p>
<p><span class="math display">\[\Gamma(n) = (n-1)! \tag{7}\]</span></p>
<p>由 <span class="math inline">\((1)\)</span> <span class="math inline">\((3)\)</span> <span class="math inline">\((6)\)</span> 很容易求得 <span class="math inline">\(\gt 0\)</span> 自然数上的Gamma函数值，由 <span class="math inline">\((2)\)</span> <span class="math inline">\((4)\)</span> <span class="math inline">\((6)\)</span> 易求得小数的Gamma函数值</p>
<h4 id="gamma-分布">Gamma 分布</h4>
<p>对于 Gamma 函数有</p>
<p><span class="math display">\[\int_{0}^{+\infty} \frac{x^{\alpha-1} e^{-x}}{\Gamma{\alpha}} dx = 1 \tag{8}\]</span></p>
<p>因此可得 Gamma 分布的密度函数为</p>
<p><span class="math display">\[Gamma(x|\alpha) = \frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)} \tag{9}\]</span></p>
<p>更一般的令 <span class="math inline">\(x = \beta t\)</span> 可得</p>
<p><span class="math display">\[Gamma(t|\alpha, \beta) = \frac{\beta^{\alpha}t^{\alpha-1}e^{-\beta t}}{\Gamma(\alpha)} \tag{10}\]</span></p>
<p><span class="math inline">\(\alpha\)</span> 决定了 Gamma 分布的曲线形状， <span class="math inline">\(\beta\)</span> 决定了 Gamma 分布曲线的陡峭程度</p>
<p><img src="/images/posts/lda_topic/gamma-distribution.png" /></p>
<h4 id="拓gamma-分布与-poisson-分布">【拓】Gamma 分布与 Poisson 分布</h4>
<p>参数为 <span class="math inline">\(\lambda\)</span> 的泊松分布概率为：</p>
<p><span class="math display">\[Poisson(X=k|\lambda) = \frac{\lambda ^{k} e-{\lambda}}{k!} \tag{11}\]</span></p>
<p>在 Gamma 分布的密度函数中取 <span class="math inline">\(\alpha = k+1\)</span> 可得</p>
<p><span class="math display">\[Gamma(x|\alpha = k+1) = \frac{x^{k}e{-x}}{\Gamma(k+1)} = \frac{x^{k}e^{-x}}{k!} \tag{12}\]</span></p>
<p>可见 <span class="math inline">\((11)\)</span> 和 <span class="math inline">\((12)\)</span> 在分布表达式上是一致的，区别在于 Poisson 分布是离散的，而 Gamma 分布是连续的</p>
<h3 id="共轭先验分布">共轭先验分布</h3>
<p>对于贝叶斯定理有</p>
<p><span class="math display">\[p(\theta | x) = \frac{p(x|\theta ) p(\theta)}{p(x)} \propto  p(x|\theta)p(\theta) \tag{13}\]</span></p>
<p>其中 <span class="math inline">\(\theta\)</span> 是参数， <span class="math inline">\(p(\theta | x)\)</span> 是后验概率，<span class="math inline">\(p(\theta)\)</span> 是先验概率，<span class="math inline">\(p(x|\theta)\)</span> 称为给定参数 <span class="math inline">\(\theta\)</span> 下样本的似然概率</p>
<p>若后验概率 <span class="math inline">\(p(\theta|x)\)</span> 和先验概率 <span class="math inline">\(p(\theta)\)</span> 满足同样的分布律，那么先验分布和后验分布叫做共轭分布，同时先验分布叫作似然函数的共轭先验分布</p>
<h3 id="二项分布和-beta-分布">二项分布和 Beta 分布</h3>
<p>对于二项分布有</p>
<p><span class="math display">\[Binomial(k|n, p) = \binom{n}{k} p^{k}(1-p)^{n-k} \tag{14}\]</span></p>
<p>Beta 分布的概率密度函数为：</p>
<p><span class="math display">\[f(x) = \begin{cases}
\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1} &amp; \text{ , } x\in [0, 1] \\ 
0 &amp; \text{ , } others
\end{cases} \tag{15}\]</span></p>
<p>其中系数 B 为</p>
<p><span class="math display">\[B(\alpha, \beta) = \int_{0}^{1} x^{\alpha -1}(1-x)^{\beta -1} dx = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)} \tag{16}\]</span></p>
<p>因此对于 Beta 分布有</p>
<p><span class="math display">\[Beta(p|\alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}p^{\alpha -1}(1-p)^{\beta - 1} \tag{17}\]</span></p>
<p>观察 <span class="math inline">\((14)\)</span> 和 <span class="math inline">\((17)\)</span> 可知两者的密度函数非常相似，尝试将两个分布联合有</p>
<p><span class="math display">\[\begin{align*}
P(p|n, k, \alpha, \beta) &amp; \propto P(k|n, p) P(p|\alpha, \beta) \\
&amp; = Binomial(k|n, p) Beta(p|\alpha, \beta) \\
&amp; = \binom{n}{k}p^{k} (1-p)^{n-k} \times \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha - 1}(1-p)^{\beta - 1} \\
&amp; \propto p^{k+\alpha - 1}(1-p)^{n-k+\beta-1}
\end{align*} \tag{18}\]</span></p>
<p>归一化后可得后验概率</p>
<p><span class="math display">\[P(p|n, k, \alpha, \beta) = \frac{\Gamma (\alpha + \beta + n)}{\Gamma(\alpha + k) \Gamma (\beta + n - k)}p^{k+\alpha-1}(1-p)^{n-k+\beta-1} \tag{19}\]</span></p>
<p>可以发现<strong>二项分布的共轭分布就是 Beta 分布</strong>，而且有</p>
<p><span class="math display">\[Beta(p|\alpha, \beta) + BinomialCount(k, n-k) = Beta(p|\alpha + k, \beta+n-k) \tag{20}\]</span></p>
<p><strong>注意！</strong> <span class="math inline">\((20)\)</span> 的 <span class="math inline">\(+\)</span> 并不是加法运算，可把它理解为<code>结合</code>，<span class="math inline">\(BinomialCount\)</span> 有数据得到的，也就是说 <span class="math inline">\((20)\)</span> 式满足 <code>先验 + 数据 = 后验</code>，称为 <code>Beta-Binomial 共轭</code></p>
<h4 id="beta分布的期望">Beta分布的期望</h4>
<p><span class="math display">\[\begin{align*}
E(Beta(p|\alpha, \beta)) &amp; = \int_{0}^{1} t Beta(p|\alpha, \beta) dt \\
&amp; = \int_{0}^{1} t \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} t^{\alpha - 1} (1-t)^{\beta - 1} dt \\
&amp; = \int_{0}^{1} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} t^{\alpha} (1-t)^{\beta - 1} dt
\end{align*} \tag{21}\]</span></p>
<p>又有</p>
<p><span class="math display">\[\int_{0}^{1} \frac{\Gamma(\alpha + \beta + 1)}{\Gamma(\alpha + 1)\Gamma(\beta)} p^{\alpha} (1-p)^{\beta-1} = 1 \tag{22}\]</span></p>
<p>由 <span class="math inline">\((21) (22)\)</span> 可得</p>
<p><span class="math display">\[E(Beta(p|\alpha, \beta)) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\ \frac{\Gamma(\alpha+1)\Gamma(\beta)}{\Gamma(\alpha+\beta+1)} = \frac{\alpha}{\alpha+\beta} \tag{23}\]</span></p>
<h3 id="多项分布和-dirichlet-分布">多项分布和 Dirichlet 分布</h3>
<p>多项分布是二项分布在多种( <span class="math inline">\(&gt;2\)</span> )状态下的推广，对于多项分布有：</p>
<p><span class="math display">\[Multinomial(\underset{m}{\rightarrow}|n, \underset{p}{\rightarrow}) = \binom{n}{\underset{m}{\rightarrow}}\prod_{k=1}^{K} p_{k}^{m_{k}} \tag{24}\]</span></p>
<p>Dirichlet 分布是 Beta 分布在多维状态下的推广，Dirichlet 分布的密度函数为</p>
<p><span class="math display">\[f(\underset{p}{\rightarrow}|\underset{\alpha}{\rightarrow}) = \begin{cases}
\frac{1}{\Delta (\underset{\alpha}{\rightarrow})} \prod_{k=1}^{K} p_{k}^{\alpha_{k-1}} &amp; \text{ , } p_{k} \in [0,1] \\ 
0 &amp; \text{ , } others 
\end{cases} \tag{25}\]</span></p>
<p>可知其密度函数的自由度为 <span class="math inline">\(k-1\)</span></p>
<p>对于系数有</p>
<p><span class="math display">\[\Delta(\underset{\alpha}{\rightarrow}) = \frac{\prod_{k=1}^{K} \Gamma(\alpha_{k})}{\Gamma(\sum_{k=1}^{K} \alpha_{k})} \tag{26}\]</span></p>
<p>因此 Dirichlet 分布可表示为</p>
<p><span class="math display">\[Dir(\underset{p}{\rightarrow}|\underset{\alpha}{\rightarrow}) = \frac{\Gamma(\sum_{k=1}^{K}\alpha _{k})}{\prod_{k=1}^{K}\Gamma(\alpha_{k})} \prod_{k-1}^{K} p_{k}^{\alpha _{k-1}} \tag{27}\]</span></p>
<p>Dirichlet 分布的期望和 Beta 分布差不多</p>
<p><span class="math display">\[E(p_{i}) = \frac{\alpha_{i}}{\sum_{k=1}^{K} \alpha_{k}} \tag{28}\]</span></p>
<p>可知<strong>多项分布是二项分布的推广</strong>，<strong>Dirichlet 分布是 Beta 分布的推广</strong></p>
<p>多项分布和 Dirichlet 分布也同样满足共轭关系，因此有</p>
<p><span class="math display">\[Dir(\underset{p}{\rightarrow} | \underset{\alpha}{\rightarrow}) + MultinomialCount(\underset{m}{\rightarrow}) = Dir(\underset{p}{\rightarrow} | \underset{\alpha}{\rightarrow} + \underset{m}{\rightarrow}) \tag{29}\]</span></p>
<p>这里的 <span class="math inline">\(+\)</span> 也是代表<code>结合</code>的意思，并不是加法，因此同样满足 <code>先验 + 数据 = 后验</code>，称为 <code>Dirichlet-Multinomial 共轭</code></p>
<h4 id="对称-dirichlet-分布">对称 Dirichlet 分布</h4>
<p>在具体实现中为了简化计算，对于参数 <span class="math inline">\(\underset{\alpha}{\rightarrow} = (\alpha _{1}, \alpha _{2}, ..., \alpha _{k})\)</span> 令 <span class="math inline">\(\alpha _{1} = \alpha _{2} = ... = \alpha _{k} = \alpha\)</span> , 满足这样的参数称为对称 Dirchlet 分布，对于 <span class="math inline">\(\alpha\)</span> 的取值有</p>
<ul>
<li><span class="math inline">\(\alpha = 1\)</span> : 退化为均匀分布</li>
<li><span class="math inline">\(\alpha \gt 1\)</span> : <span class="math inline">\(p_{1} = p_{2} = ... = p_{k}\)</span> 的概率增大</li>
<li><span class="math inline">\(\alpha \lt 1\)</span> : <span class="math inline">\(p_{i}=1\)</span>, <span class="math inline">\(p_{\neq i} = 0\)</span> 的概率增大</li>
</ul>
<img src="/images/posts/lda_topic/dirichlet-alpha.gif" />
<p align="center">
图片来自维基百科
</p>
<h3 id="硬-软思维">硬 &amp; 软思维</h3>
<p>举个例子，判断一个人的性别</p>
<ul>
<li>硬分类思想：这个人是男的（很果断，非0即1）</li>
<li>软分类思想：这个人 70% 的可能是男的， 30% 的可能是女的，所以他应该是个男的</li>
</ul>
<h2 id="再话-lda">再话 LDA</h2>
<p>在 <code>LDA 的解释</code> 部分已经解释了 LDA 的基本思想，这里结合背景知识再深入的讲解</p>
<p>假设有 M 篇文档，对应第 d 个文档种有 <span class="math inline">\(N_{d}\)</span> 词</p>
<p><img src="/images/posts/lda_topic/corpus.png" /></p>
<p>我们的目标是找到每一篇文档的主题分布和每一个主题中词的分布。</p>
<p>我们假设<code>隐变量K</code>代表主题数，则 LDA 的概率图模型为</p>
<p><img src="/images/posts/lda_topic/bayes-model.png" /></p>
<p>图中只有 <code>Observed Word</code> 是可见的，其他都是<code>隐</code>的，<code>Observed word</code> <span class="math inline">\(W _{d, n}\)</span> 代表第 d 个文档的第 n 个词</p>
<p>LDA 假设文档的主题的先验分布是 Dirichlet 分布，即对于任一文档 d ，其主题分布 <span class="math inline">\(\theta _{d}\)</span> 为</p>
<p><span class="math display">\[\theta _{d} = Dir(\underset{\alpha}{\rightarrow})\]</span></p>
<p>其中 <span class="math inline">\(\alpha \in \mathbb{R}^{K}\)</span> 是分布的超参数，是一个 <span class="math inline">\(K\)</span> 维向量（它对应了文档分别属于 <span class="math inline">\(K\)</span> 个主题的软概率）</p>
<p>LDA 假设主题中词的先验分布是 Dirichlet 分布，即对于任一主题 <span class="math inline">\(k\)</span>，其词分布 <span class="math inline">\(\beta _{k}\)</span> 为</p>
<p><span class="math display">\[\beta_{k} = Dir(\underset{\eta }{\rightarrow})\]</span></p>
<p>其中 <span class="math inline">\(\eta \in \mathbb{R}^{V}\)</span> 为分布的超参数，<span class="math inline">\(V\)</span> 代表词典的大小， 是一个 <span class="math inline">\(V\)</span> 维的向量（它对应了词典中各个词对主题影响的软概率）</p>
<p>对于数据中任意一篇文档 <span class="math inline">\(d\)</span> 中的第 n 个词，可以从主题分布 <span class="math inline">\(\theta_{d}\)</span> 中得到它的主题编号 <span class="math inline">\(z_{d,n}\)</span> 的分布为</p>
<p><span class="math display">\[z_{d,n} = Multinomial(\theta_{d})\]</span></p>
<p>而对于该主题编号，可以得到我们看到词 <span class="math inline">\(w_{d, n}\)</span> 的概率分布为</p>
<p><span class="math display">\[w_{d, n} = Multinomial(\beta_{z_{d, n}})\]</span></p>
<p>对于 M 篇文档就有 M 个文档主题的 Dirichlet 分布，而对应的<strong>数据</strong>就有 M 个主题编号的多项分布，因此 <span class="math inline">\(\alpha \rightarrow \theta_{d} \rightarrow \underset{z_{d}}{\rightarrow}\)</span> 满足 Dirichlet-Multinomial 共轭</p>
<p>那么数据似然部分，如何求解呢？</p>
<p>假设在第 k 个主题中，第 v 个词的个数为 <span class="math inline">\(n_{k}^{(v)}\)</span>，则该主题编号对应的词多项分布计数可表示为</p>
<p><span class="math display">\[\underset{n_{k}}{\rightarrow} = (n_{k}^{(1)}, n_{k}^{(2)}, ..., n_{k}^{(V)})\]</span></p>
<p>利用 Dirichlet-Multinomial 共轭，可得 <span class="math inline">\(\beta_{k}\)</span> 的后验分布为</p>
<p><span class="math display">\[Dir(\beta_{k}|\underset{\eta}{\rightarrow}+\underset{n_{k}}{\rightarrow})\]</span></p>
<p><strong>由于主题产生词不依赖具体的某一个文档，因此文档主题分布和词分布是独立的</strong></p>
<p>可能上面的解释还是难以理解，下面尝试以更通俗的语言解释</p>
<blockquote>
<p>这里的超参数一般是我们根据经验设定一个具体的数，这样也就是得到了我们的最原始的先验分布 初始化：利用先验分布，为每一个文档 m 中的每一个词 n 赋予一个主题 z 根据这个初始化 我们可以统计文档m的各个主题数目，也可以统计某个主题z对应的不同词的数目 这些数目我们看做第一步的数据似然 -&gt; 然后再结合先验 -&gt; 得到第一步的后验 把第一步的后验看做第二步的先验概率再去为每一个文档 m 中的每一个词 n 赋予一个主题 z，因为是共轭先验分布因此进行和第一步一样的迭代，直到收敛 —— 上述解释来自 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6831308.html">kylin0228</a></p>
</blockquote>
<p>本文只介绍原理部分，实现部分后期发布</p>
<h2 id="refrence">Refrence</h2>
<ul>
<li>[1] https://en.wikipedia.org/wiki/Dirichlet_distribution</li>
<li>[2] http://www.flickering.cn/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/2014/06/lda%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6%E6%96%87%E6%9C%AC%E5%BB%BA%E6%A8%A1/</li>
<li>[3] https://www.cnblogs.com/pinard/p/6831308.html</li>
<li>[4] http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2018/10/20/tensorflow%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E4%BB%A5%E5%8F%8A%E6%8A%80%E5%B7%A7/" rel="prev" title="tensorflow常用函数以及技巧">
                  <i class="fa fa-chevron-left"></i> tensorflow常用函数以及技巧
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2018/11/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%82%A3%E4%BA%9B%E4%BA%8B/" rel="next" title="深度学习那些事">
                  深度学习那些事 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sean Lee</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">156k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:22</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.4.0/pjax.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollRestoration: false,
  scrollTo: !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  






  



    <div class="pjax">

  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



    </div>
</body>
</html>
